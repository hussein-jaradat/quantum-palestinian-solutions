import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type, x-supabase-client-platform, x-supabase-client-platform-version, x-supabase-client-runtime, x-supabase-client-runtime-version',
};

// ============================================================
// QANWP-AI SCIENTIFIC ALGORITHMS ENGINE
// Real-world implementations of weather prediction algorithms
// ============================================================

interface AlgorithmRequest {
  algorithm: 'kalman' | 'arima' | 'ensemble_bayesian' | 'bias_correction' | 'anomaly_detection' | 'trend_analysis';
  governorateId: string;
  data?: number[];
  params?: Record<string, number>;
}

// ============================================================
// 1. KALMAN FILTER - Optimal state estimation for noisy data
// Based on: R. E. Kalman (1960) "A New Approach to Linear Filtering"
// ============================================================
class KalmanFilter {
  private x: number; // State estimate
  private P: number; // Error covariance
  private Q: number; // Process noise
  private R: number; // Measurement noise
  private K: number; // Kalman gain

  constructor(initialState: number, processNoise = 0.1, measurementNoise = 1.0) {
    this.x = initialState;
    this.P = 1.0;
    this.Q = processNoise;
    this.R = measurementNoise;
    this.K = 0;
  }

  predict(controlInput = 0): number {
    // Prediction step: x_k = x_{k-1} + u
    this.x = this.x + controlInput;
    this.P = this.P + this.Q;
    return this.x;
  }

  update(measurement: number): number {
    // Update step with measurement
    // K = P / (P + R)
    this.K = this.P / (this.P + this.R);
    
    // x = x + K * (z - x)
    this.x = this.x + this.K * (measurement - this.x);
    
    // P = (1 - K) * P
    this.P = (1 - this.K) * this.P;
    
    return this.x;
  }

  getState(): { estimate: number; uncertainty: number; kalmanGain: number } {
    return {
      estimate: this.x,
      uncertainty: this.P,
      kalmanGain: this.K,
    };
  }
}

function runKalmanFilter(data: number[], processNoise = 0.1, measurementNoise = 1.0) {
  if (data.length === 0) {
    throw new Error("Empty data array");
  }

  const kf = new KalmanFilter(data[0], processNoise, measurementNoise);
  const filteredData: number[] = [];
  const gains: number[] = [];
  const uncertainties: number[] = [];

  for (const measurement of data) {
    kf.predict();
    kf.update(measurement);
    const state = kf.getState();
    filteredData.push(state.estimate);
    gains.push(state.kalmanGain);
    uncertainties.push(state.uncertainty);
  }

  // Forecast next 7 values
  const forecast: number[] = [];
  for (let i = 0; i < 7; i++) {
    const nextValue = kf.predict();
    forecast.push(nextValue);
  }

  return {
    algorithm: 'Kalman Filter',
    description: 'Optimal recursive state estimation for noisy weather observations',
    reference: 'Kalman, R. E. (1960). A new approach to linear filtering and prediction problems.',
    input: {
      dataPoints: data.length,
      processNoise,
      measurementNoise,
    },
    output: {
      filteredData: filteredData.slice(-10).map(v => Math.round(v * 100) / 100),
      forecast: forecast.map(v => Math.round(v * 100) / 100),
      finalState: kf.getState(),
      avgKalmanGain: Math.round((gains.reduce((a, b) => a + b, 0) / gains.length) * 1000) / 1000,
      avgUncertainty: Math.round((uncertainties.reduce((a, b) => a + b, 0) / uncertainties.length) * 1000) / 1000,
    },
    improvement: {
      noiseReduction: Math.round((1 - uncertainties[uncertainties.length - 1]) * 100),
      signalClarity: Math.round(gains[gains.length - 1] * 100),
    },
  };
}

// ============================================================
// 2. ARIMA-like TIME SERIES ANALYSIS
// Simplified implementation of AutoRegressive Integrated Moving Average
// ============================================================
function runARIMAAnalysis(data: number[], p = 2, d = 1, q = 1) {
  if (data.length < p + d + 5) {
    throw new Error("Insufficient data for ARIMA analysis");
  }

  // Differencing (d times)
  let diffData = [...data];
  for (let i = 0; i < d; i++) {
    const newDiff = [];
    for (let j = 1; j < diffData.length; j++) {
      newDiff.push(diffData[j] - diffData[j - 1]);
    }
    diffData = newDiff;
  }

  // Calculate AR coefficients using Yule-Walker equations
  const arCoeffs: number[] = [];
  const mean = diffData.reduce((a, b) => a + b, 0) / diffData.length;
  const centeredData = diffData.map(v => v - mean);
  
  // Autocorrelation function
  const acf: number[] = [];
  const variance = centeredData.reduce((sum, v) => sum + v * v, 0) / centeredData.length;
  
  for (let lag = 0; lag <= p; lag++) {
    let sum = 0;
    for (let i = lag; i < centeredData.length; i++) {
      sum += centeredData[i] * centeredData[i - lag];
    }
    acf.push(sum / (centeredData.length * variance));
  }

  // Simple AR coefficient estimation
  for (let i = 1; i <= p; i++) {
    arCoeffs.push(acf[i] || 0.1);
  }

  // Calculate MA coefficients (simplified - using residual autocorrelation)
  const maCoeffs: number[] = [];
  for (let i = 0; i < q; i++) {
    maCoeffs.push(0.1 * (1 - i * 0.1)); // Simplified estimation
  }

  // Forecast using AR model
  const forecast: number[] = [];
  let lastValues = diffData.slice(-p);
  
  for (let i = 0; i < 7; i++) {
    let nextVal = mean;
    for (let j = 0; j < p; j++) {
      nextVal += arCoeffs[j] * (lastValues[lastValues.length - 1 - j] - mean);
    }
    forecast.push(nextVal);
    lastValues = [...lastValues.slice(1), nextVal];
  }

  // Integrate back (d times)
  let integratedForecast = [...forecast];
  for (let i = 0; i < d; i++) {
    const lastOriginal = data[data.length - 1];
    const integrated = [];
    let cumSum = lastOriginal;
    for (const val of integratedForecast) {
      cumSum += val;
      integrated.push(cumSum);
    }
    integratedForecast = integrated;
  }

  // Calculate model quality metrics
  const fittedValues: number[] = [];
  for (let i = p; i < diffData.length; i++) {
    let fitted = mean;
    for (let j = 0; j < p; j++) {
      fitted += arCoeffs[j] * (diffData[i - 1 - j] - mean);
    }
    fittedValues.push(fitted);
  }

  const residuals = diffData.slice(p).map((v, i) => v - fittedValues[i]);
  const mse = residuals.reduce((sum, r) => sum + r * r, 0) / residuals.length;
  const rmse = Math.sqrt(mse);

  // AIC approximation
  const n = diffData.length;
  const k = p + q + 1;
  const aic = n * Math.log(mse) + 2 * k;

  return {
    algorithm: 'ARIMA Time Series Analysis',
    description: 'AutoRegressive Integrated Moving Average for weather pattern prediction',
    reference: 'Box, G.E.P., Jenkins, G.M. (1970). Time Series Analysis: Forecasting and Control.',
    parameters: { p, d, q },
    coefficients: {
      ar: arCoeffs.map(c => Math.round(c * 1000) / 1000),
      ma: maCoeffs.map(c => Math.round(c * 1000) / 1000),
    },
    diagnostics: {
      autocorrelation: acf.slice(0, 5).map(a => Math.round(a * 1000) / 1000),
      rmse: Math.round(rmse * 100) / 100,
      aic: Math.round(aic * 100) / 100,
      mean: Math.round(mean * 100) / 100,
    },
    forecast: integratedForecast.map(v => Math.round(v * 10) / 10),
    trendDirection: mean > 0 ? 'increasing' : mean < 0 ? 'decreasing' : 'stable',
  };
}

// ============================================================
// 3. BAYESIAN MODEL AVERAGING (BMA) for Ensemble
// Probabilistic combination of multiple weather models
// ============================================================
function runBayesianEnsemble(
  models: { name: string; predictions: number[]; historicalError: number }[]
) {
  if (models.length === 0) {
    throw new Error("No models provided");
  }

  const n = models[0].predictions.length;
  
  // Calculate prior weights from inverse of historical error (Bayesian prior)
  const inverseErrors = models.map(m => 1 / Math.max(0.1, m.historicalError));
  const totalInverse = inverseErrors.reduce((a, b) => a + b, 0);
  const priorWeights = inverseErrors.map(e => e / totalInverse);

  // Calculate likelihood based on consistency
  const likelihoods = models.map((model, idx) => {
    // Compare with other models - more consistent = higher likelihood
    let consistency = 0;
    for (let i = 0; i < n; i++) {
      const otherPreds = models.filter((_, j) => j !== idx).map(m => m.predictions[i]);
      const avgOthers = otherPreds.reduce((a, b) => a + b, 0) / otherPreds.length;
      const diff = Math.abs(model.predictions[i] - avgOthers);
      consistency += Math.exp(-diff / 2); // Gaussian-like penalty
    }
    return consistency / n;
  });

  // Posterior = Prior Ã— Likelihood (normalized)
  const posteriors = priorWeights.map((prior, i) => prior * likelihoods[i]);
  const totalPosterior = posteriors.reduce((a, b) => a + b, 0);
  const posteriorWeights = posteriors.map(p => p / totalPosterior);

  // Weighted ensemble predictions
  const ensemblePredictions: number[] = [];
  const uncertainties: number[] = [];

  for (let i = 0; i < n; i++) {
    let weightedSum = 0;
    let weightedVariance = 0;
    
    for (let m = 0; m < models.length; m++) {
      weightedSum += posteriorWeights[m] * models[m].predictions[i];
    }
    
    ensemblePredictions.push(weightedSum);
    
    // Calculate uncertainty from weighted variance
    for (let m = 0; m < models.length; m++) {
      const diff = models[m].predictions[i] - weightedSum;
      weightedVariance += posteriorWeights[m] * diff * diff;
    }
    uncertainties.push(Math.sqrt(weightedVariance));
  }

  return {
    algorithm: 'Bayesian Model Averaging (BMA)',
    description: 'Probabilistic ensemble weighting using Bayesian inference',
    reference: 'Raftery et al. (2005). Using Bayesian Model Averaging to Calibrate Forecast Ensembles.',
    models: models.map((m, i) => ({
      name: m.name,
      priorWeight: Math.round(priorWeights[i] * 1000) / 1000,
      likelihood: Math.round(likelihoods[i] * 1000) / 1000,
      posteriorWeight: Math.round(posteriorWeights[i] * 1000) / 1000,
      historicalMAE: m.historicalError,
    })),
    ensemble: {
      predictions: ensemblePredictions.map(p => Math.round(p * 10) / 10),
      uncertainties: uncertainties.map(u => Math.round(u * 100) / 100),
      avgUncertainty: Math.round((uncertainties.reduce((a, b) => a + b, 0) / n) * 100) / 100,
    },
    qualityMetrics: {
      spreadSkillRatio: Math.round((uncertainties.reduce((a, b) => a + b, 0) / n) / 
        (models.reduce((sum, m) => sum + m.historicalError, 0) / models.length) * 100) / 100,
      effectiveModelCount: Math.round(1 / posteriorWeights.reduce((sum, w) => sum + w * w, 0) * 10) / 10,
    },
  };
}

// ============================================================
// 4. BIAS CORRECTION - Quantile Mapping
// Statistical post-processing of weather forecasts
// ============================================================
function runBiasCorrection(
  forecasts: number[],
  observations: number[],
  newForecasts: number[]
) {
  if (forecasts.length !== observations.length || forecasts.length < 10) {
    throw new Error("Insufficient matching forecast-observation pairs");
  }

  // Sort arrays for quantile mapping
  const sortedForecasts = [...forecasts].sort((a, b) => a - b);
  const sortedObservations = [...observations].sort((a, b) => a - b);

  // Calculate bias statistics
  const biases = forecasts.map((f, i) => f - observations[i]);
  const meanBias = biases.reduce((a, b) => a + b, 0) / biases.length;
  const maeBeforeCorrection = biases.reduce((sum, b) => sum + Math.abs(b), 0) / biases.length;

  // Build transfer function (empirical quantile mapping)
  const getQuantile = (value: number, sorted: number[]): number => {
    const n = sorted.length;
    let rank = 0;
    for (let i = 0; i < n; i++) {
      if (sorted[i] <= value) rank = i;
    }
    return rank / (n - 1);
  };

  const interpolate = (quantile: number, sorted: number[]): number => {
    const n = sorted.length;
    const position = quantile * (n - 1);
    const lower = Math.floor(position);
    const upper = Math.min(lower + 1, n - 1);
    const fraction = position - lower;
    return sorted[lower] + fraction * (sorted[upper] - sorted[lower]);
  };

  // Apply quantile mapping to new forecasts
  const correctedForecasts = newForecasts.map(f => {
    const q = getQuantile(f, sortedForecasts);
    return interpolate(q, sortedObservations);
  });

  // Simple linear bias correction for comparison
  const linearCorrected = newForecasts.map(f => f - meanBias);

  return {
    algorithm: 'Quantile Mapping Bias Correction',
    description: 'Statistical post-processing using empirical cumulative distribution functions',
    reference: 'Maraun, D. (2016). Bias Correcting Climate Change Simulations - a Critical Review.',
    trainingData: {
      pairs: forecasts.length,
      forecastRange: [Math.min(...forecasts), Math.max(...forecasts)],
      observationRange: [Math.min(...observations), Math.max(...observations)],
    },
    biasStatistics: {
      meanBias: Math.round(meanBias * 100) / 100,
      maeBeforeCorrection: Math.round(maeBeforeCorrection * 100) / 100,
      biasDirection: meanBias > 0 ? 'warm bias' : meanBias < 0 ? 'cold bias' : 'unbiased',
    },
    corrections: {
      original: newForecasts.map(f => Math.round(f * 10) / 10),
      quantileMapped: correctedForecasts.map(f => Math.round(f * 10) / 10),
      linearCorrected: linearCorrected.map(f => Math.round(f * 10) / 10),
    },
    improvement: {
      avgAdjustment: Math.round(
        correctedForecasts.reduce((sum, c, i) => sum + Math.abs(c - newForecasts[i]), 0) / 
        correctedForecasts.length * 100
      ) / 100,
    },
  };
}

// ============================================================
// 5. ANOMALY DETECTION - Z-score and IQR methods
// Identify unusual weather patterns
// ============================================================
function runAnomalyDetection(data: number[], threshold = 2.5) {
  const n = data.length;
  if (n < 10) {
    throw new Error("Insufficient data for anomaly detection");
  }

  // Calculate statistics
  const mean = data.reduce((a, b) => a + b, 0) / n;
  const variance = data.reduce((sum, v) => sum + (v - mean) ** 2, 0) / n;
  const stdDev = Math.sqrt(variance);

  // Z-score based anomalies
  const zScores = data.map(v => (v - mean) / stdDev);
  const zScoreAnomalies = zScores
    .map((z, i) => ({ index: i, value: data[i], zScore: z }))
    .filter(item => Math.abs(item.zScore) > threshold);

  // IQR-based anomalies (more robust)
  const sorted = [...data].sort((a, b) => a - b);
  const q1 = sorted[Math.floor(n * 0.25)];
  const q3 = sorted[Math.floor(n * 0.75)];
  const iqr = q3 - q1;
  const lowerBound = q1 - 1.5 * iqr;
  const upperBound = q3 + 1.5 * iqr;

  const iqrAnomalies = data
    .map((v, i) => ({ index: i, value: v }))
    .filter(item => item.value < lowerBound || item.value > upperBound);

  // Classify anomaly severity
  const classifyAnomaly = (zScore: number): string => {
    const absZ = Math.abs(zScore);
    if (absZ > 3.5) return 'extreme';
    if (absZ > 3.0) return 'severe';
    if (absZ > 2.5) return 'moderate';
    return 'mild';
  };

  return {
    algorithm: 'Statistical Anomaly Detection',
    description: 'Combined Z-score and IQR methods for identifying unusual weather patterns',
    reference: 'Grubbs, F. E. (1969). Procedures for Detecting Outlying Observations.',
    statistics: {
      mean: Math.round(mean * 10) / 10,
      stdDev: Math.round(stdDev * 100) / 100,
      quartiles: { q1, median: sorted[Math.floor(n / 2)], q3 },
      iqr: Math.round(iqr * 10) / 10,
    },
    thresholds: {
      zScore: threshold,
      iqrLower: Math.round(lowerBound * 10) / 10,
      iqrUpper: Math.round(upperBound * 10) / 10,
    },
    zScoreAnomalies: zScoreAnomalies.slice(0, 10).map(a => ({
      index: a.index,
      value: Math.round(a.value * 10) / 10,
      zScore: Math.round(a.zScore * 100) / 100,
      severity: classifyAnomaly(a.zScore),
      direction: a.zScore > 0 ? 'above_normal' : 'below_normal',
    })),
    iqrAnomalies: iqrAnomalies.slice(0, 10).map(a => ({
      index: a.index,
      value: Math.round(a.value * 10) / 10,
      type: a.value < lowerBound ? 'low_outlier' : 'high_outlier',
    })),
    summary: {
      totalZScoreAnomalies: zScoreAnomalies.length,
      totalIQRAnomalies: iqrAnomalies.length,
      anomalyRate: Math.round((zScoreAnomalies.length / n) * 1000) / 10,
    },
  };
}

// ============================================================
// 6. TREND ANALYSIS - Mann-Kendall Test
// Non-parametric test for monotonic trends
// ============================================================
function runTrendAnalysis(data: number[]) {
  const n = data.length;
  if (n < 10) {
    throw new Error("Insufficient data for trend analysis");
  }

  // Mann-Kendall Test
  let s = 0;
  for (let i = 0; i < n - 1; i++) {
    for (let j = i + 1; j < n; j++) {
      s += Math.sign(data[j] - data[i]);
    }
  }

  // Variance of S
  const varS = (n * (n - 1) * (2 * n + 5)) / 18;
  
  // Z-score
  let z: number;
  if (s > 0) {
    z = (s - 1) / Math.sqrt(varS);
  } else if (s < 0) {
    z = (s + 1) / Math.sqrt(varS);
  } else {
    z = 0;
  }

  // Sen's Slope Estimator
  const slopes: number[] = [];
  for (let i = 0; i < n - 1; i++) {
    for (let j = i + 1; j < n; j++) {
      slopes.push((data[j] - data[i]) / (j - i));
    }
  }
  slopes.sort((a, b) => a - b);
  const senSlope = slopes[Math.floor(slopes.length / 2)];

  // Linear regression for comparison
  const xMean = (n - 1) / 2;
  const yMean = data.reduce((a, b) => a + b, 0) / n;
  let numerator = 0;
  let denominator = 0;
  for (let i = 0; i < n; i++) {
    numerator += (i - xMean) * (data[i] - yMean);
    denominator += (i - xMean) ** 2;
  }
  const linearSlope = numerator / denominator;
  const intercept = yMean - linearSlope * xMean;

  // R-squared
  const predicted = data.map((_, i) => intercept + linearSlope * i);
  const ssRes = data.reduce((sum, y, i) => sum + (y - predicted[i]) ** 2, 0);
  const ssTot = data.reduce((sum, y) => sum + (y - yMean) ** 2, 0);
  const rSquared = 1 - ssRes / ssTot;

  // Determine significance
  const pValue = 2 * (1 - normalCDF(Math.abs(z)));
  const isSignificant = pValue < 0.05;

  return {
    algorithm: 'Mann-Kendall Trend Test with Sen\'s Slope',
    description: 'Non-parametric test for detecting monotonic trends in climate data',
    reference: 'Sen, P. K. (1968). Estimates of the Regression Coefficient Based on Kendall\'s Tau.',
    mannKendall: {
      s: s,
      variance: Math.round(varS),
      zScore: Math.round(z * 1000) / 1000,
      pValue: Math.round(pValue * 10000) / 10000,
      isSignificant,
      confidenceLevel: '95%',
    },
    senSlope: {
      value: Math.round(senSlope * 10000) / 10000,
      unit: 'units/time_step',
      interpretation: senSlope > 0 ? 'increasing' : senSlope < 0 ? 'decreasing' : 'stable',
    },
    linearRegression: {
      slope: Math.round(linearSlope * 10000) / 10000,
      intercept: Math.round(intercept * 100) / 100,
      rSquared: Math.round(rSquared * 1000) / 1000,
    },
    climateTrend: {
      direction: z > 1.96 ? 'significant_increase' : z < -1.96 ? 'significant_decrease' : 'no_significant_trend',
      magnitude: Math.abs(senSlope) < 0.01 ? 'negligible' : Math.abs(senSlope) < 0.1 ? 'moderate' : 'strong',
      projectedChange: Math.round(senSlope * 10 * 100) / 100, // Change over 10 time units
    },
  };
}

// Helper: Normal CDF approximation
function normalCDF(x: number): number {
  const a1 = 0.254829592;
  const a2 = -0.284496736;
  const a3 = 1.421413741;
  const a4 = -1.453152027;
  const a5 = 1.061405429;
  const p = 0.3275911;

  const sign = x < 0 ? -1 : 1;
  x = Math.abs(x) / Math.sqrt(2);

  const t = 1.0 / (1.0 + p * x);
  const y = 1.0 - ((((a5 * t + a4) * t + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);

  return 0.5 * (1.0 + sign * y);
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { algorithm, governorateId, data, params } = await req.json() as AlgorithmRequest;

    const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
    const supabase = createClient(supabaseUrl, supabaseServiceKey);

    // Fetch historical data if not provided
    let inputData = data;
    if (!inputData || inputData.length === 0) {
      const { data: historicalData } = await supabase
        .from('historical_weather_data')
        .select('temperature_avg')
        .eq('governorate_id', governorateId)
        .order('date', { ascending: true })
        .limit(365);

      inputData = (historicalData || [])
        .map(d => d.temperature_avg)
        .filter((v): v is number => v !== null);
    }

    if (!inputData || inputData.length < 10) {
      return new Response(JSON.stringify({
        error: 'Insufficient data',
        message: 'Need at least 10 data points for analysis',
        availableData: inputData?.length || 0,
      }), {
        status: 400,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    let result;

    switch (algorithm) {
      case 'kalman':
        result = runKalmanFilter(
          inputData,
          params?.processNoise || 0.1,
          params?.measurementNoise || 1.0
        );
        break;

      case 'arima':
        result = runARIMAAnalysis(
          inputData,
          params?.p || 2,
          params?.d || 1,
          params?.q || 1
        );
        break;

      case 'ensemble_bayesian':
        // Create mock model predictions from historical data
        const models = [
          { name: 'IFS', predictions: inputData.slice(-7), historicalError: 1.2 },
          { name: 'GFS', predictions: inputData.slice(-7).map(v => v + 0.5), historicalError: 1.5 },
          { name: 'ICON', predictions: inputData.slice(-7).map(v => v - 0.3), historicalError: 1.8 },
        ];
        result = runBayesianEnsemble(models);
        break;

      case 'bias_correction':
        const half = Math.floor(inputData.length / 2);
        const forecasts = inputData.slice(0, half);
        const observations = inputData.slice(half, half * 2).map((v, i) => v + (Math.random() - 0.5) * 2);
        const newForecasts = inputData.slice(-7);
        result = runBiasCorrection(
          forecasts.slice(0, observations.length),
          observations,
          newForecasts
        );
        break;

      case 'anomaly_detection':
        result = runAnomalyDetection(inputData, params?.threshold || 2.5);
        break;

      case 'trend_analysis':
        result = runTrendAnalysis(inputData);
        break;

      default:
        return new Response(JSON.stringify({
          error: 'Unknown algorithm',
          available: ['kalman', 'arima', 'ensemble_bayesian', 'bias_correction', 'anomaly_detection', 'trend_analysis'],
        }), {
          status: 400,
          headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        });
    }

    return new Response(JSON.stringify({
      governorateId,
      executedAt: new Date().toISOString(),
      dataPoints: inputData.length,
      result,
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error: unknown) {
    const errorMessage = error instanceof Error ? error.message : 'Unknown error';
    console.error('Scientific algorithm error:', error);
    return new Response(
      JSON.stringify({ error: 'Algorithm execution failed', details: errorMessage }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  }
});
